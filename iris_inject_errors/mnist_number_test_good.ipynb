{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd191f5-a9d8-4e56-89a3-da98fd3b5c49",
   "metadata": {},
   "source": [
    "### Messing with trying to get error injection to work with .fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a20f16-1bc0-4a9f-aad8-e36a05435086",
   "metadata": {},
   "source": [
    "#### first try using a custom model with train step override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0912b9fd-723f-4968-bd08-b886a52065e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import error_inject_layer\n",
    "import dense_error_injection\n",
    "import error_inject_optimizer\n",
    "from tensorflow.python.util import deprecation\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d23883-e808-4e3f-bad9-73314b15fa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21501243-6473-466f-b906-6d9473f2b57e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c46d8e9-8596-44d8-9b24-9cc77524681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c21c3b-fde4-464c-a878-2bd05dc4b336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.keras.datasets.mnist' from '/home/derek/projects/ml/tf_env/lib/python3.8/site-packages/tensorflow/keras/datasets/mnist/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4d975a-4b57-4406-bf47-9e3450394876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd2cfe-7536-4bd9-ae75-326159a9734c",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20479841-f5eb-4bea-8e03-f406f91e5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale values\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e041a6-feb6-47cc-8343-ac88d997364b",
   "metadata": {},
   "source": [
    "## Create Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa229503-c325-4bb9-9749-00df87516449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea8349f3-e99a-4464-92f2-66c605f85436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94d63473-1aa7-44db-9542-b6548ebe01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, y):\n",
    "\t# keep track of our gradients\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\t# make a prediction using the model and then calculate the\n",
    "\t\t# loss\n",
    "\t\tpred = model(X)\n",
    "\t\tloss = categorical_crossentropy(y, pred)\n",
    "\t# calculate the gradients using our tape and then update the\n",
    "\t# model weights\n",
    "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
    "\topt.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d17163c4-c078-44c1-a0a8-6188372c2b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n",
      "[INFO] creating model...\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for, batch size, and\n",
    "# initial learning rate\n",
    "EPOCHS = 25\n",
    "BS = 32\n",
    "INIT_LR = 1e-3\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "# trainX = np.expand_dims(trainX, axis=-1)\n",
    "# testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "# one-hot encode the labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# build our model and initialize our optimizer\n",
    "print(\"[INFO] creating model...\")\n",
    "# model = build_model(28, 28, 1, 10)\n",
    "opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e0c6d5e-9f98-4ef5-bb0f-ab63b8ff3afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "numUpdates = int(trainX.shape[0] / BS)\n",
    "print(numUpdates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "174ea7ef-5d99-416d-9c19-9c54afa757d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting epoch 0/25...\n",
      "[INFO] starting epoch 1/25...\n",
      "[INFO] starting epoch 2/25...\n",
      "[INFO] starting epoch 3/25...\n",
      "[INFO] starting epoch 4/25...\n",
      "[INFO] starting epoch 5/25...\n",
      "[INFO] starting epoch 6/25...\n",
      "[INFO] starting epoch 7/25...\n",
      "[INFO] starting epoch 8/25...\n",
      "[INFO] starting epoch 9/25...\n",
      "[INFO] starting epoch 10/25...\n",
      "[INFO] starting epoch 11/25...\n",
      "[INFO] starting epoch 12/25...\n",
      "[INFO] starting epoch 13/25...\n",
      "[INFO] starting epoch 14/25...\n",
      "[INFO] starting epoch 15/25...\n",
      "[INFO] starting epoch 16/25...\n",
      "[INFO] starting epoch 17/25...\n",
      "[INFO] starting epoch 18/25...\n",
      "[INFO] starting epoch 19/25...\n",
      "[INFO] starting epoch 20/25...\n",
      "[INFO] starting epoch 21/25...\n",
      "[INFO] starting epoch 22/25...\n",
      "[INFO] starting epoch 23/25...\n",
      "[INFO] starting epoch 24/25...\n"
     ]
    }
   ],
   "source": [
    "# compute the number of batch updates per epoch\n",
    "numUpdates = int(trainX.shape[0] / BS)\n",
    "# loop over the number of epochs\n",
    "for epoch in range(0, EPOCHS):\n",
    "\t# show the current epoch number\n",
    "\tprint(f\"[INFO] starting epoch {epoch}/{EPOCHS}...\")\n",
    "# \tsys.stdout.flush()\n",
    "# \tepochStart = time.time()\n",
    "\t# loop over the data in batch size increments\n",
    "\tfor i in range(0, numUpdates):\n",
    "\t\t# determine starting and ending slice indexes for the current\n",
    "\t\t# batch\n",
    "\t\tstart = i * BS\n",
    "\t\tend = start + BS\n",
    "\t\t# take a step\n",
    "\t\tstep(trainX[start:end], trainY[start:end])\n",
    "\t# show timing information for the epoch\n",
    "# \tepochEnd = time.time()\n",
    "# \telapsed = (epochEnd - epochStart) / 60.0\n",
    "# \tprint(\"took {:.4} minutes\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e3c8a51-baa5-49c5-8dd9-29d8c5dfb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0965 - acc: 0.9803\n",
      "[INFO] test accuracy: 0.9803\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=opt, loss=categorical_crossentropy,\n",
    "\tmetrics=[\"acc\"])\n",
    "# now that the model is compiled we can compute the accuracy\n",
    "(loss, acc) = model.evaluate(testX, testY)\n",
    "print(\"[INFO] test accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0bba9-25ef-4986-aa70-b228400840a7",
   "metadata": {},
   "source": [
    "### Maybe sparse categorical cross entropy was the problem with custom training\n",
    "\n",
    "Should retry everything with categorical cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894abf14-0b06-4ae7-b894-666b21e747d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
